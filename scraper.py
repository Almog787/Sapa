import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import pytz
import pandas as pd
import time

# --- ×”×’×“×¨×•×ª: ×¨×©×™××ª ××•×¦×¨×™× ×œ××¢×§×‘ ---
# ×›××Ÿ ××ª×” ××•×¡×™×£ ××ª ×”×§×™×©×•×¨×™× ×©××¦×× ×•
PRODUCTS = [
    {"url": "https://www.ace.co.il/5760921", "name": "ACE - Leader Sofa"},
    {"url": "https://www.zilberahit.co.il/product/%D7%A1%D7%A4%D7%94-%D7%A4%D7%99%D7%A0%D7%AA%D7%99%D7%AA-%D7%9C%D7%99%D7%93%D7%A8-leader/", "name": "Zilber - Leader Sofa"},
    # ××¤×©×¨ ×œ×”×•×¡×™×£ ×¢×•×“ ×§×™×©×•×¨×™× ×›××Ÿ
]

DATA_FILE = "data.json"
README_FILE = "README.md"
TZ_ISRAEL = pytz.timezone('Asia/Jerusalem')

def get_price_from_url(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        price = "0"
        
        # ×œ×•×’×™×§×” ×œ××ª×¨×™× ×©×•× ×™×
        if "ace.co.il" in url:
            price_meta = soup.find('meta', property='product:price:amount')
            if price_meta: price = price_meta['content']
            else:
                span = soup.find('span', {'data-price-type': 'finalPrice'})
                if span: price = span.get_text(strip=True)
                
        elif "zilberahit" in url:
             # ×”×ª×××” ×œ×–×™×œ×‘×¨ (×¦×¨×™×š ×œ×•×•×“× ××ª ×”-Class ×‘××ª×¨ ×©×œ×”×, ×–×” × ×™×—×•×© ××•×©×›×œ)
             # ×œ×¨×•×‘ ×‘×•×•×§×•××¨×¡/×•×•×¨×“×¤×¨×¡ ×–×” × ×¨××” ×›×š:
             price_tag = soup.find('p', class_='price')
             if price_tag:
                 ins = price_tag.find('ins') # ××—×™×¨ ××‘×¦×¢
                 if ins: price = ins.get_text(strip=True)
                 else: price = price_tag.get_text(strip=True)

        elif "shufersal" in url:
            # ×”×ª×××” ×œ×©×•×¤×¨×¡×œ
            price_div = soup.find('span', class_='priceText')
            if price_div: price = price_div.get_text(strip=True)

        # × ×™×§×•×™ ×”××—×™×¨
        clean_price = ''.join(filter(lambda x: x.isdigit() or x == '.', str(price)))
        return float(clean_price) if clean_price else 0.0

    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def update_database(products_list):
    # ×˜×¢×™× ×ª × ×ª×•× ×™× ×§×™×™××™×
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            try:
                db = json.load(f)
            except: db = {}
    else:
        db = {}

    timestamp = datetime.now(TZ_ISRAEL).strftime("%Y-%m-%d %H:%M:%S")
    
    # ×¨×™×¦×” ×¢×œ ×›×œ ×”××•×¦×¨×™×
    for prod in products_list:
        url = prod['url']
        name = prod['name']
        price = get_price_from_url(url)
        
        if price is not None:
            if url not in db:
                db[url] = {"name": name, "history": []}
            
            # ×”×•×¡×¤×ª ×“×’×™××”
            db[url]["history"].append({
                "timestamp": timestamp,
                "price": price
            })
            
            # ×©××™×¨×” ×¢×œ ×”×™×¡×˜×•×¨×™×” ×¡×‘×™×¨×” (×œ××©×œ 500 ××—×¨×•× ×™×)
            db[url]["history"] = db[url]["history"][-500:]
            print(f"Scraped {name}: {price}")
        else:
            print(f"Failed to scrape {name}")
        
        time.sleep(2) # × ×™××•×¡ ×œ××ª×¨×™× (×”×©×”×™×™×” ×§×¦×¨×”)

    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(db, f, ensure_ascii=False, indent=4)
    
    return db

def generate_readme(db):
    if not db: return

    md = "# ğŸ¤– ×‘×•×˜ ×”×©×•×•××ª ××—×™×¨×™×\n\n"
    md += f"**×¢×“×›×•×Ÿ ××—×¨×•×Ÿ:** {datetime.now(TZ_ISRAEL).strftime('%d/%m/%Y %H:%M')}\n\n"
    
    # ×˜×‘×œ×” ××¡×›××ª
    md += "## ğŸ† ×˜×‘×œ×ª ×”×©×•×•××” × ×•×›×—×™×ª\n"
    md += "| ×©× ×”××•×¦×¨ | ××—×™×¨ ××—×¨×•×Ÿ | ×©×™× ×•×™ |\n|---|---|---|\n"
    
    for url, data in db.items():
        if not data['history']: continue
        latest = data['history'][-1]
        price = latest['price']
        name = data['name']
        
        # ×—×™×©×•×‘ ×©×™× ×•×™
        change_icon = "â–"
        if len(data['history']) > 1:
            prev = data['history'][-2]['price']
            if price < prev: change_icon = "ğŸ”» ×™×¨×™×“×”"
            elif price > prev: change_icon = "ğŸ”º ×¢×œ×™×”"
            
        md += f"| [{name}]({url}) | â‚ª{price} | {change_icon} |\n"

    md += "\n---\n"
    
    # ×¤×™×¨×•×˜ ×œ×›×œ ××•×¦×¨
    for url, data in db.items():
        if not data['history']: continue
        md += f"### ğŸ“Š ×”×™×¡×˜×•×¨×™×”: {data['name']}\n"
        md += "| ×ª××¨×™×š | ××—×™×¨ |\n|---|---|\n"
        for entry in reversed(data['history'][-10:]): # 10 ××—×¨×•× ×™×
            md += f"| {entry['timestamp']} | â‚ª{entry['price']} |\n"
        md += "\n"

    with open(README_FILE, 'w', encoding='utf-8') as f:
        f.write(md)

if __name__ == "__main__":
    db = update_database(PRODUCTS)
    generate_readme(db)
